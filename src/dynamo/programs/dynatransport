#!/usr/bin/env python2
#   dynamo:- Event driven molecular dynamics simulator 
#   http://www.dynamomd.org
#   Copyright (C) 2009  Marcus N Campbell Bannerman <m.bannerman@gmail.com>
#
#   This program is free software: you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   version 3 as published by the Free Software Foundation.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
import os
import math
import xml.etree.ElementTree as ET
import numpy as np

#Function to load a (possibly compressed) xml file.
def loadXMLFile(filename):
    #Check if the file is compressed or not, and 
    if (os.path.splitext(filename)[1][1:].strip() == "bz2"):
        import bz2
        f = bz2.BZ2File(filename)
        doc = ET.parse(f)
        f.close()
        return doc
    else:
        return ET.parse(filename)

#Command line argument parsing
from optparse import OptionParser
parser = OptionParser()
parser.add_option("-c", "--cutoff-time", dest="cutofftime", type="float", default=1e300,
                  help="The TIME beyond which data from the correlators are discarded.", metavar="TIME", )
parser.add_option("-s", "--start-time", dest="starttime",
                  help="The amount of TIME to discard data at the start of the correlator.", 
                  type="float", default=0, metavar="TIME")

parser.add_option("-t", "--term", dest="term", type="int", default=0,
                   help="Return the (0) full correlator, (1) the free-streaming/continuous self correlation CC, (2) the free-streaming/continuous and interaction cross correlation CI, (3) the interaction and free-streaming cross correlation IC, (4) the interaction self correlation II")


parser.add_option("-v", "--view-fit", action="store_true", default=False,
                  dest="plot", help="View the fits of the correlators")

(options, datafiles) = parser.parse_args()

#Sort out the data file option
if len(datafiles) == 0:
    print("Need to supply at least one input file!")
    import sys
    sys.exit()

def parseToArray(text):
    data = []
    for line in text.strip().split("\n"):
        dataline = list(map(float, line.split()))
        if (len(dataline) > 1):
            data.append(dataline)
    return np.array(data)

def avg(data):
    if len(data) == 0:
        raise ArithmeticError("Can only find the average of a list with non-zero length")
    return sum(data) / (len(data))

def stddev(data):
    average = avg(data)
    return avg([(item - average)**2 for item in data])

def fitData(datacolumns, full_data, filename, autoremove_quad=False, quadterm=0, offset=0):
    import numpy as np

    #Filter the region to fit to
    data = [line for line in full_data if (line[0] >= options.starttime) and (line[0] <= options.cutofftime)]
    
    quadratic_terms = [np.poly1d([quadterm, -offset, 0])]*len(datacolumns)
    if autoremove_quad:
        #Fit quadratics to the whole dataset to guess the curvature
        
        # This just takes the last data point and assumes its the quadratic
        xmax = full_data[-1][0]
        quadratic_terms = [np.poly1d([full_data[-1][column] / xmax / xmax, -offset, 0]) for column in datacolumns]

        #from numpy.polynomial import Polynomial
        #quadratic_terms=[]
        #for col in datacolumns:
        #    fit = np.polyfit([row[0] for row in data], [row[col] for row in data], 2)
        #    curve = np.poly1d([fit[0], -offset,0])
        #    quadratic_terms.append(curve)
        
    #Collect the x values
    xvalues = [line[0] for line in data]

    #Get all the y values, and remove the linear offset and the quadratic term
    ys = [[line[column] - q(line[0]) for column, q in zip(datacolumns, quadratic_terms)] for line in data]
    #print(ys)
    #Now average the final correlator
    yvalues = [avg(y) for y in ys]
    
    #And calculate the deviation
    yerrvalues = [math.sqrt(stddev(y)) for y in ys]

    fitdata = np.polyfit(xvalues, yvalues, 1, full=True)
    slope = fitdata[0][0]
    intercept = fitdata[0][1]
    SSreg=stddev(np.polyval(fitdata[0],xvalues) - yvalues)
    SStot=stddev(yvalues)
    if SSreg == 0.0 and SStot == 0.0:
        R2 = 1.0
    else:
        R2 = 1.0 - SSreg/SStot
    return slope, intercept, R2, xvalues, yvalues, yerrvalues, filename

shear_viscosity_data = []
longitudinal_viscosity_data = []
bulk_viscosity_data = []
cross_viscosity_data = []

thermalconductivity_data = []
thermaldiffusion_data = {}
mutualdiffusion_data = {}
for filename in datafiles:
    if not os.path.exists(filename):
        print("Could not find the passed datafile! ("+filename+")")
        import sys
        sys.exit()

    #Load the xml file
    XMLDoc = loadXMLFile(filename)
    RootElement=XMLDoc.getroot()

    #Parse all of the transport data (and draw the plots if requested)
    def parseComponents(matches):
        data = None
        for xmlelement in matches:
            if options.term != 0:
                if options.term == 1:
                    if xmlelement.attrib['type'] != 'CC':
                        continue
                elif options.term == 2:
                    if xmlelement.attrib['type'] != 'CI':
                        continue
                elif options.term == 3:
                    if xmlelement.attrib['type'] != 'IC':
                        continue
                elif options.term == 4:
                    if xmlelement.attrib['type'] != 'II':
                        continue
                else:
                    print(repr(options.term))
                    raise Exception("Bad value for the --term option")
            
            if data is None:
                data = parseToArray(xmlelement.text)
            else:
                data[:,2:] = data[:,2:] + parseToArray(xmlelement.text)[:,2:]
        return data

    pressure = float(RootElement.find(".//Pressure").attrib['Avg'])
    image = RootElement.find(".//PrimaryImageSimulationSize")
    x = float(image.attrib['x'])
    y = float(image.attrib['y'])
    z = float(image.attrib['z'])
    V = x * y * x
    N = int(RootElement.find(".//ParticleCount").attrib['val'])
    RT = float(RootElement.find(".//Temperature").attrib['Mean'])

    p_excess = pressure - (N/V) * RT
    
    for ele in RootElement.findall(".//Viscosity/Correlator"):
        viscdata = parseComponents(ele.findall(".//Component"))
        shear_viscosity = fitData([3, 4, 7], viscdata, filename)
        shear_viscosity_data.extend([shear_viscosity])
        shear_visc, intercept, R2, xvalues, yvalues, yerrvalues, filename = shear_viscosity
        
        #longitudinal_viscosity_data.extend([fitData([2, 6, 10], viscdata, filename, autoremove_quad=False, quadterm=p_excess**2 * V / 2, offset=-(4.0/3.0) * shear_visc)])
        longitudinal_viscosity_data.extend([fitData([2, 6, 10], viscdata, filename, autoremove_quad=False, offset=0)])
        
    for ele in RootElement.findall(".//BulkViscosity/Correlator"):
        viscdata = parseComponents(ele.findall(".//Component"))
        bulk_viscosity_data.extend([fitData([2], viscdata, filename, autoremove_quad=False, offset=0)])
 
    for ele in RootElement.findall(".//CrossViscosity/Correlator"):
        viscdata = parseComponents(ele.findall(".//Component"))
        cross_viscosity_data.extend([fitData([2, 3, 4], viscdata, filename, autoremove_quad=False, offset=0)])
       
    for ele in RootElement.findall(".//ThermalConductivity/Correlator"):
        thermdata = parseComponents(ele.findall(".//Component"))
        thermalconductivity_data.extend([fitData([2, 3, 4], thermdata, filename)])

    for ele in RootElement.findall(".//ThermalDiffusion/Correlator"):
        key = ele.attrib["Species"]
        thermdata = parseComponents(ele.findall(".//Component"))
        thermaldiffusion_data.setdefault(key, []).extend([fitData([2, 3, 4], thermdata, filename)])

    for ele in RootElement.findall(".//MutualDiffusion/Correlator"):
        key=ele.attrib["Species1"]+","+ele.attrib["Species2"]
        diffdata = parseComponents(ele.findall(".//Component"))
        mutualdiffusion_data.setdefault(key, []).extend([fitData([2, 3, 4], diffdata, filename)])


def output(title, correlator_data):
    avgslope = avg([data[0] for data in correlator_data])
    stddevslope = math.sqrt(stddev([data[0] for data in correlator_data]))
    avgintercept = avg([data[1] for data in correlator_data])
    avgR2 = avg([data[2] for data in correlator_data])
    print(title, avgslope, "+-", stddevslope, "<R>^2=", avgR2)
    #Plotting output
    minx=1e300
    maxx=0
    if (options.plot):
        import matplotlib.pyplot as plt
        plt.title("$"+title+str(avgslope)+"$ $R^2="+str(avgR2)+"$")
        for slope, intercept, R2, xvalues, yvalues, yerrvalues, filename in correlator_data:
            minx = min(min(xvalues), minx)
            maxx = max(max(xvalues), maxx)
            plt.errorbar(xvalues, yvalues, yerr=yerrvalues, fmt='x', label=filename)
        plt.plot([minx, maxx], [minx * avgslope + avgintercept, maxx * avgslope + avgintercept])
        plt.legend()
        plt.show()
    return avgslope

output("ShearViscosityL_{xyxy}=", shear_viscosity_data)
output("LongitudinalViscosityL_{xxxx}=", longitudinal_viscosity_data)
output("BulkViscosityL_{Tr}=", bulk_viscosity_data)
output("CrossViscosityL_{xxyy}=", cross_viscosity_data)
output("ThermalConductivityL_{\\lambda,\\lambda}=", thermalconductivity_data)

for species, data in thermaldiffusion_data.items():
    output("ThermalDiffusionL_{\\lambda,"+species+"}=",data)

for key, data in mutualdiffusion_data.items():
    output("MutualDiffusionL_{"+key+"}=",data)
